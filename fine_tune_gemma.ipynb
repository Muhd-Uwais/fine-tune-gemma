{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac9cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning Gemma Model with Transformers and custom local dataset\n",
    "# This script is designed to run in a Google Colab environment.\n",
    "\n",
    "# !pip uninstall bitsandbytes peft trl accelerate transformers datasets\n",
    "# The above line is commented out, but it's often used to ensure a clean environment\n",
    "# before installing specific versions of libraries, especially in environments like Colab.\n",
    "\n",
    "# Install necessary libraries for fine-tuning\n",
    "!pip3 install bitsandbytes # For 8-bit or 4-bit quantization, reducing memory usage\n",
    "!pip3 install peft # Parameter-Efficient Fine-Tuning library (e.g., LoRA)\n",
    "!pip3 install trl # Transformer Reinforcement Learning, useful for SFT (Supervised Fine-Tuning)\n",
    "!pip3 install accelerate # Speeds up model training on different hardware setups\n",
    "!pip3 install datasets # Hugging Face's library for easily loading and processing datasets\n",
    "!pip3 install transformers # Core Hugging Face library for pre-trained models and training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5354872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # For interacting with the operating system, like setting environment variables\n",
    "import transformers # Hugging Face Transformers library\n",
    "import torch # PyTorch library, essential for deep learning operations\n",
    "from datasets import load_dataset, Dataset # For handling datasets\n",
    "from trl import SFTTrainer # Trainer specifically designed for Supervised Fine-Tuning\n",
    "from peft import LoraConfig, PeftModel, PeftConfig, get_peft_model # PEFT configurations and utilities\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM # For loading tokenizer and language model\n",
    "from transformers import BitsAndBytesConfig, GemmaTokenizer # Specific configurations for quantization and Gemma tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6260ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login() # Logs into Hugging Face Hub, required for downloading gated models, provide your api_key from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b55fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model ID for the Gemma 3-1B instruction-tuned model\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# Configure BitsAndBytes for 4-bit quantization\n",
    "# This significantly reduces memory footprint, allowing larger models or batch sizes\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # Load the model weights in 4-bit precision\n",
    "    bnb_4bit_quant_type=\"nf4\", # Specifies the 4-bit quantization type (NormalFloat 4-bit)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # The data type used for computation during fine-tuning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04541887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for the specified model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# Load the pre-trained Causal Language Model with quantization configuration\n",
    "# device_map=\"auto\" automatically distributes the model across available devices (e.g., GPUs)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config,\n",
    "                                                device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e9320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable Weights & Biases (WandB) logging if not needed.\n",
    "# If you want to track experiments, set this to \"true\" or remove the line.\n",
    "os.environ[\"WANDB_DISABLED\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cebda64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA (Low-Rank Adaptation) for Parameter-Efficient Fine-Tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank of the update matrices. A smaller 'r' means fewer trainable parameters.\n",
    "    # Specifies which modules (layers) in the model will have LoRA applied.\n",
    "    # These are typically attention and feed-forward layers in transformer models.\n",
    "    target_modules={\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"},\n",
    "    task_type=\"CAUSAL_LM\", # Defines the task type as Causal Language Modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dae2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset # Import Dataset class\n",
    "import json # For handling JSON data\n",
    "\n",
    "# Read the JSONL (JSON Lines) file manually\n",
    "# This is a common format for datasets where each line is a separate JSON object.\n",
    "file_path = \"<user_dataset>.jsonl\" # Placeholder for your dataset file path\n",
    "data = []\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.strip())) # Parse each line as a JSON object and add to list\n",
    "\n",
    "# Convert the list of dictionaries into a HuggingFace Dataset object\n",
    "datasett = Dataset.from_list(data)\n",
    "\n",
    "# Optional: Check a sample from the dataset to verify content\n",
    "print(datasett[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d92169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to format the dataset examples for Supervised Fine-Tuning (SFT)\n",
    "# The SFTTrainer expects a 'text' column containing the formatted input for the model.\n",
    "def format_for_sft(example):\n",
    "    # This format creates a clear instruction-response pair for the model to learn from.\n",
    "    return {\n",
    "        \"text\": f\"Instruction:{example['instruction']}\\nResponse:{example['context']}\"\n",
    "    }\n",
    "\n",
    "# Apply the formatting function to the dataset\n",
    "formatted_dataset = datasett.map(format_for_sft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb72fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SFTTrainer for fine-tuning the model\n",
    "trainer = SFTTrainer(\n",
    "    model=model, # The model to be fine-tuned\n",
    "    processing_class=tokenizer, # The tokenizer to use for processing input text\n",
    "    train_dataset=formatted_dataset, # The formatted dataset for training\n",
    "    args=transformers.TrainingArguments( # Configuration for the training process\n",
    "        per_device_train_batch_size=8, # Number of samples per batch per device\n",
    "        gradient_accumulation_steps=16, # Accumulate gradients over multiple steps to simulate a larger batch size\n",
    "        warmup_steps=2, # Number of steps for learning rate warmup\n",
    "        num_train_epochs=1, # Number of full passes over the training data\n",
    "        # max_steps=300, # Alternative to num_train_epochs, for training for a fixed number of steps\n",
    "        learning_rate=2e-4, # Initial learning rate\n",
    "        fp16=True, # Enable mixed-precision training (float16) for faster training and less memory\n",
    "        logging_steps=1, # Log metrics every 1 step\n",
    "        output_dir=\"<output_save_dir>\", # Directory to save checkpoints and logs\n",
    "        optim=\"paged_adamw_8bit\", # Optimizer to use (AdamW with 8-bit paging for memory efficiency)\n",
    "    ),\n",
    "    peft_config=lora_config, # Apply the LoRA configuration during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33cf163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6dcc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned PEFT (LoRA) model weights and tokenizer\n",
    "trainer.model.save_pretrained(\"<output_saved_dir>\")     # Saves the LoRA adapters\n",
    "trainer.tokenizer.save_pretrained(\"<output_saved_dir>\") # Saves the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92122ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig, get_peft_model # Re-importing specific PEFT components for clarity\n",
    "from transformers import AutoModelForCausalLM # Re-importing for clarity\n",
    "\n",
    "# Load the PEFT configuration from the saved directory\n",
    "peft_model_id = \"<output_saved_dir>\" # The directory where your LoRA adapters were saved\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# Load the base model again (without quantization this time, as we will merge)\n",
    "# config.base_model_name_or_path holds the original model_id\n",
    "base_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "# Load the PEFT model by applying the LoRA adapters on top of the base model\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "\n",
    "# Merge the LoRA weights into the base model.\n",
    "# This creates a single, consolidated model that can be used for inference\n",
    "# without needing the PEFT library.\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save the merged full model (base model + merged LoRA weights)\n",
    "dir_to_save = \"<merged_model_dir>\"\n",
    "merged_model.save_pretrained(dir_to_save) # Directory to save the fully merged model\n",
    "tokenizer.save_pretrained(dir_to_save) # Save the tokenizer again with the merged model\n",
    "\n",
    "print(f\"‚úÖ Merged full model saved at '{dir_to_save}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed09338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the merged model and tokenizer for inference\n",
    "model_id = \"<merged_model_dir>\" # Alternatively, you can use the \"dir_to_save\" variable from the previous step\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) # Load tokenizer from merged model directory\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id) # Load the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b530579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to CUDA (GPU) if available, otherwise to CPU\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158370b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt for testing the fine-tuned model\n",
    "prompt = \"Instruction: What is the history of internet\\nResponse:\"\n",
    "# Tokenize the prompt and move it to the appropriate device (CUDA/CPU)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(inputs) # Print the tokenized inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response using the fine-tuned model\n",
    "outputs = model.generate(**inputs, max_new_tokens=50) # Generate up to 50 new tokens\n",
    "# Decode the generated tokens back into human-readable text\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa757f4",
   "metadata": {},
   "source": [
    "##### <b><i>The below part only for those who want to quantize the model.</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9171a667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps to convert the Hugging Face model to GGUF format using llama.cpp\n",
    "# This is useful for running the model on a CPU or with llama.cpp's optimizations.\n",
    "\n",
    "# ‚úÖ Step 1: Go to the root directory in Colab's file system\n",
    "%cd /content\n",
    "\n",
    "# ‚úÖ Step 2: Clone llama.cpp repository (fresh clone to ensure latest version)\n",
    "!rm -rf llama.cpp # Remove any existing llama.cpp directory\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git # Clone the repository\n",
    "%cd llama.cpp # Change directory into the cloned llama.cpp\n",
    "\n",
    "# ‚úÖ Step 3: Build llama.cpp with CMake\n",
    "!mkdir -p build # Create a build directory\n",
    "%cd build # Change into the build directory\n",
    "!cmake .. # Configure the project with CMake\n",
    "!make -j # Compile the project using all available CPU cores\n",
    "%cd .. # Go back to the llama.cpp root directory\n",
    "\n",
    "# ‚úÖ Step 4: Convert merged Hugging Face model to GGUF format (specific for Gemma)\n",
    "# This script converts the Hugging Face model (which is in PyTorch format) to the GGUF format.\n",
    "# Ensure that '/content/merged_model/' contains the saved merged model files (safetensors, config.json, etc.).\n",
    "# Change the \"merged_model\" with your merged model directory.\n",
    "!python3 convert_hf_to_gguf.py /content/merged_model --outfile /content/gemma3.gguf \n",
    "\n",
    "# ‚úÖ Step 5: Quantize the GGUF model to 4-bit (q4_0 quantization)\n",
    "# This further reduces the model size and memory usage, suitable for CPU inference.\n",
    "# './build/bin/llama-quantize' is the executable compiled in Step 3.\n",
    "!./build/bin/llama-quantize /content/gemma3.gguf /content/gemma3-q4.gguf q4_0 # Change gemma3-q4.gguf to your desired output file name\n",
    "\n",
    "# ‚úÖ Step 6: Verify the output by listing the file size of the quantized GGUF model\n",
    "!ls -lh /content/gemma3-q4.gguf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b241617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now download the model from google colab file section\n",
    "\"\"\" \n",
    "After running this notebook, you can find 'gemma3-q4.gguf' or \n",
    "the file with your given name in the Colab file browser \n",
    "\"\"\"\n",
    "# (usually on the left sidebar) under the '/content/' directory. You can then download it.\n",
    "# Thank You! üôè"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
